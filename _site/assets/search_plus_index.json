{"/pages/Backoff/": {
    "title": "Backoff in N-Gram LMs",
    "keywords": "Smoothing",
    "url": "/pages/Backoff/",
    "body": "In Homework 1, you are tasked with fitting a model using real backoff. J&amp;M introduce the idea of backoff (substituting the extremely underspecified n-gram MLE probability estimates with the (likely less sparse) lower order estimates), but only provides the details of what they call stupid backoff, which gives up on the idea of having returning real probabilities and just returns the probability an MLE (n-1)-Gram model would return (if it’s non-zero, of course — if it is 0 we back off further. See the textbook for the details!). The reason the textbook gives up on returning real probabilities because, to keep it brief, making the math work out is tricky. This also happens to be why I ask you to implement it for real! Figure out how the math works, implement it, and then understand why no one wants to do this. The other reason I ask you to do this is because it acts as a good test of whether you understand some of the probabilistic reasoning behind N-Gram Modeling, which sounds less cruel. Backing Off Note that I will be intentionally terse with the idea behind backoff. Make sure you understand the discussion of backoff from the reading first! So, the first key idea is that stupid backoff will fail to be a probability distribution because the conditional ``probability distribution’’ estimated by the model will often sum to over $1$ — Not a real probability distribution after all. Consider that if we consider only $n$-grams with count $&gt;0$, we will have an MLE estimate of the probability distribution that will sum to 1. Recall that this estimate is… \\[\\begin{aligned} p(w_k \\mid w_{k-n+1} \\dots w_{k-1}) &amp;= \\frac{c(w_{k-n+1} \\dots w_k)}{\\sum_{w_k \\in V}c(w_{k-n+1} \\dots w_k)} \\\\ \\sum_{w_k \\in V} p(w_k \\mid w_{k-n+1} \\dots w_{k-1}) &amp;= \\sum_{w_k \\in V}\\frac{c(w_{k-n+1} \\dots w_k)}{\\sum_{w_k \\in V}c(w_{k-n+1} \\dots w_k)} = 1 \\end{aligned}\\] If we add any more probability to n-grams with $0$ counts, we’ll have this sum go above 1! This is why stupid backoff does not allow us to have a real probability distribution. Our solution in real backoff is to apply discounting: We’ll take some probability from n-grams with non-zero probability in our MLE estimates and give that to those with 0 probability. How much we reserve will be a hyperparameter of our model. In the instructions, I suggest you discount the MLE estimates by a factor of $0 &lt; \\delta &lt; 1$, and reserve $1 - \\delta$ for unseen n-grams. Thus, we partition the sample space into two pieces: the probability of $w_k$s where we’ve seen the corresponding n-gram (that is, $w_k$ where $c(w_{k-n+1} \\dots w_{k}) &gt; 0$) and those which we haven’t seen ($c(w_{k-n+1}\\dots w_k) = 0$). For the ones we have seen, we use the MLE estimates above and just add the discount factor! \\[\\begin{aligned} p(w_k \\mid w_{k-n+1} \\dots w_{k-1}) &amp;= \\delta\\frac{c(w_{k-n+1 \\dots w_k})}{\\sum_{w_k \\in V}c(w_{k-n+1} \\dots w_k)} \\\\ \\sum_{w_k \\in V} p(w_k \\mid w_{k-n+1} \\dots w_{k-1}) &amp;= \\sum_{w_k \\in V}\\delta\\frac{c(w_{k-n+1 \\dots w_k})}{\\sum_{w_k \\in V}c(w_{k-n+1} \\dots w_k)} = \\delta \\end{aligned}\\] So we need the sum for all unseen $w_k$s to sum to $1-\\delta$. There is a subtle trick here if you try and take too many shortcuts in assembling this equation though: You can’t just take the MLE estimate of the (n-1)-gram model! Consider that the (n-1)-gram MLE model will have $\\sum_{w_k \\in V} p(w_k \\mid w_{k-n+2} \\dpts w_{k-1) = 1$, but we are only summing over $w_k$ if $c(w_{k-n+1} \\dots w_{k-1}) = 0$. That means we might come in under $1$ when we sum over all $w_k \\in V$ for our backoff model. The trick is to not try and be clever, and just work out the right probabilities from first principles: We want $\\sum_{w_k \\in V, c(w_{k-n+1} \\dots w_{k-1}) = 0} p_{bo}(w_k \\mid w_{k-n+2} \\dots w_{k-1}) = 1$. We want each probability to be proportional to the (n-1)-gram count, so we also want $p_{bo}(w_k \\mid w_{k-n+2} \\dots w_{k-1}) = \\alpha c(w_{k-n+2} \\dots w_{k-1})$ for some $\\alpha$. Now just do a little algebra to find $\\alpha$: \\[\\begin{aligned} \\sum_{w_k \\in V, c(w_{k-n+1} \\dots w_{k}) = 0} \\alpha c(w_{k-n+2} \\dots w_{k-1}) &amp;= 1 \\\\ \\alpha \\sum_{w_k \\in V, c(w_{k-n+1} \\dots w_{k}) = 0} c(w_{k-n+2} \\dots w_{k-1}) &amp;= 1 \\\\ \\alpha &amp;= \\frac{1}{\\sum_{w_k \\in V, c(w_{k-n+1} \\dots w_{k}) = 0} c(w_{k-n+2} \\dots w_{k-1})} \\end{aligned}\\] Now that’s not a nice thing to compute, but this (typos aside — let me know!) should give you a real conditional probabilitiy distribution. Now, you just need to incorporate multiple stages of backoff. What if the relevant (n-1)-gram has no observations, we should backoff even further! Discount the (n-1)-gram backoff estimate (so we weight $p_{bo}$ not by $(1-\\delta)$, but by $\\delta(1-\\delta)$, reserving that last $(1-\\delta)^2$ for the lower order ($1\\dots(n-1)$-gram) backoff models. This is what the instructions are getting at when they give you the $\\delta(1-\\delta)^{m-1}$ figure! Good luck coding, and please come to OH or ask for help as you come closer to the deadline!"
  },"/pages/Unit1Extensions/": {
    "title": "Unit 1: Additional Readings and Extensions",
    "keywords": "papers classical nlp",
    "url": "/pages/Unit1Extensions/",
    "body": "For each unit, I’ll try to compile a list of resources/further readings in case a topic we covered is of particular interest to you! For the most part, what we cover will only skim the surface of what is out there. The aim of this page is to guide you towards reasonable next steps in exploring these topics that should be challenging, but accessible given what we’ve covered. N-Gram Language Modeling If you were to train an n-gram model of any sort in practice, you would not want to roll your own implementation (i.e., don’t actually use the code you wrote for HW1!). This is mainly because when you work with large models, you need to be very careful with efficiency lest your models take up massive amounts of time and memory to train (imagine the counts for a large $n$ and a massive corpus). In practice, there are two libraries folks use: KenLM and SRiLM, both with fair use and usable documentation/tutorialization. N-Gram models are primarily useful as a historical and pedagogical tool, but n-gram-like ideas do show up in modern research in a number of ways! One way is by scaling them up to the dataset scale we have today. infini-gram is one recent proposal that builds a novel backoff system and trillions of tokens. Interpolating with Llama reduces model perplexity! N-Gram models also find purchase by being easily interpretable (unlike neural models) while being somewhat easy to build (unlike PCFGs). This let’s them be a useful model for, say, a reference model for analyzing the computational expressivity of a transformer, as a stepping stone to help guide the training of a fancier model, as part of the evaluation of the performance of machine translation systems, as a part of models for low-resource languages where larger models are not as feasible. I know them most comfortably as a direct measure of attested next-word probability when doing cognitive modeling or another computational linguistics-y task (see Smith \\&amp; Levy (2013), or for something more recent like Shain et al. (2024) where they often show up as a baseline. This is just a tiny sampling — they show up as baselines everywhere! This is not to even mention their use in the social science/digital humanities where interpretability is also critical (see, for instance, something like Baklāne \\&amp; Saulespurēns (2023) which seems very cool!). Note that I gathered these through some very light searching for recent papers, primarily from the ACL Anthology, which collects papers from the conferences it hosts (that are considered top NLP conferences!). The smoothing techniques we saw in class are relatively simple, and were primarily to show you the two basic approaches to doing smoothing: interpolation-based methods and backoff-based methods. More complex techniques abound, and the definitive reference for these methods is Chen &amp; Goodman 1996, which surveys their empirical performance and finds that Modified Kneser-Ney Smoothing has the best performance. This is usually the default option when doing n-gram smoothing nowadays. That’s not to say that add-1/Laplace-style smoothing is entirely irrelevant. It was very recently found that add-k smoothing is formally equivalent to label smoothing, a technique for regularizing some neural language models (Malagutti et al. 2024. The paper also shows a method for developing new regularizers based on other n-gram smoothing techniques. Tokenization BPE is a very popular tokenization technique, but has a number of alternatives like Wordpiece (Schuster &amp; Nakajima 2012) and UnigramLM. Tokenization tends to be introduced through papers that aim to do something bigger than just tokenization (even if that is the legacy they leave!), so I recommend looking towards broader survey articles like Mielke et al. (2021) to get a lay of the land and a description of the key differences between methods. There are many reasonable ways to tokenize in practice, but a convenient and easy-to-use go-to for reference implementations is those on HuggingFace. The linked tutorial is a good introduction to start playing with tokenizers! Tokenization is tricky business in practice, both in terms of developing a vocabulary and determining how to tokenize sequences when that becomes ambiguous. BPE does this by incrementally building a vocabulary by merging frequent bigrams, and then applying merges in the order they were done during training. Different tokenization algorithms vary in how they execute both steps — pay attention to this if you’re looking into different tokenization schemes! The relationship between tokenization and morphological decomposition (i.e., breaking words into meaningful units) is complex. You can look at how weird real tokenization ends up like this paper, or you can check how aspects of tokenization map onto psychological measures of word processing like this paper or this paper. CFGs and Parsing One issue we didn’t talk much about was how we get probabilities on a CFG (or even build a CFG in the first place). The empirical answer is from Treebanks — we have linguists annotate (or create annotation guidelines) for a large corpus, and then we estimate what the probabilities should be in order to, say, maximize the likelihood of generating the treebank from the grammar. The most common treebank in English is the Penn Treebank, a small portion of which is accessible in NLTK (as I believe HW0 demonstrated). If you don’t have any parses available, the task of determining the grammar is called grammar induction, and is a hard problem! It is also quite neat because it mirrors (in some way) one possibility for how humans come to acquire a mental grammar (i.e., rules about what is and isn’t a good sentence of our languages), since we know this even before we might have taken a class that teaches usa prescriptive grammar! Something like Klein &amp; Manning (2001) is probably a good start to get thinking about that! CFGs are one of many kinds of grammars one can use to model language. Others include Combinatory Categorial Grammar, or CCG, which is both used widely in NLP and has serious linguistic theorizing, Dependency Grammars, which eschew constituency as an idea in favor of directly modeling grammatical relations (like agreement, subject-hood, etc.), and some like Tree-Adjoining Grammars, which are a little bit more linguistics facing. To get a lot more linguistics facing, you can look at a formalism like Minimalist Grammar, which directly attempts to encode syntactic theorizing in linguistics in a formal grammar system. This unfortunately means that the link I provide is both the most accessible intro I could find and highly technical! One thing notable about CCG and TAG is that they are more expressive than CFGs. What that means is that there are some (formal) languages that can be defined by CCG/TAG that cannot be defined by CFGs. For this reason, they are called (Mildy) Context-Sensitive Languages. This notion is above and beyond an undergraduate curriculum, but these are the kinds of questions we get interested in as we talk about formal language theory in a class like Theory of Computation (Offered this Spring (2025)!). Also worth noting that some human languages have been shown to not be expressible by context-free grammars, most notably Swiss-German and Dutch, so modeling human linguistic competence with CFGs is already a futile game. The issue, broadly, of understanding what the right structure underlying language is is within the domain of syntactic theory, a field in linguistics. Sans taking a full linguistics class, the reference I recommend is the one I was taught out of – Carnie’s Syntax: A Generative Introduction. One thing you should know about linguistics is that everything in syntax is highly controversial, especially the ideas behind generative syntax stemming from Chomsky’s research program. To avoid claims of impropriety, I will also provide references for alternative approaches, like Optimality-Theoretic Syntax, or HPSG. Parsing is a broad issue that spans Formal Language Theory, NLP/Linguistics, and Compiler Design. As a result, a variety of different parsing strategies beyond what we’ve looked at are particularly relevant in various fields. The most notable missing pieces are a discussion of Shift-Reduce Parsing, a simple strategy for parsing that finds use both in neural NLP and in compilers, Earley Parsing (Earley 1970, Stolcke 1995), a similar incremental parsing algorithm that’s often used to compute prefix-probabilities to make PCFGs usable conditional language models, and a discussion of Left-Corner Parsing, a strategy that merges some of the ideas behind top-down and bottom-up parsing to build more efficient parsers (like the left-corner filter in Stolcke’s Earley Parsing Implementation!). Earley Parsing in particular is influential in computational linguistics. Hale (2001) proposes using a probabilistic Earley parser as a model of human sentence processing (or at least that of Garden Path sentences!). The critical factor here is incrementality: being able to partially parse before you get the full string when you receive your input strictly from left to right."
  }}
